import os
import cv2
import numpy as np
import torch
import glob
import shutil
import pandas as pd
from sam2.build_sam import build_sam2_video_predictor

# ===================== 1. é…ç½®å‚æ•° =====================
clips_dir = "/home/itaer2/zxy/cataract/code/sam2-main/video_clip_external"
checkpoint = "./checkpoints/sam2.1_hiera_large.pt"
model_cfg = "configs/sam2.1/sam2.1_hiera_l.yaml"
output_root = "./outputs_clips_25fps_external"
os.makedirs(output_root, exist_ok=True)

# YOLO æ£€æµ‹ç»“æœ CSV æ–‡ä»¶
yolo_csv_path = "/home/itaer2/zxy/cataract/code/sam2-main/sam2/auto_sam2_box_prompts_external.csv"
fps = 25
clip_length_sec = 60
clip_length_frames = int(fps * clip_length_sec)

# çœ¼éƒ¨æç¤ºç‚¹ï¼ˆä¸­å¿ƒç‚¹åæ ‡ï¼‰- æ ¹æ®åŸå§‹æ¡†è®¡ç®—
# åŸå§‹æ¡† [206, 2, 1095, 672] çš„ä¸­å¿ƒç‚¹: (x_center, y_center)
eye_center = np.array([[(206 + 1095) / 2, (2 + 672) / 2]], dtype=np.float32)  # å½¢çŠ¶ä¸º (1, 2)
eye_label = np.array([1], dtype=np.int64)  # 1 è¡¨ç¤ºå‰æ™¯

# ===================== 2. ä» CSV è¯»å–æ£€æµ‹æ¡†ç”Ÿæˆ Prompt =====================
df_boxes = pd.read_csv(yolo_csv_path)
print(f"ğŸ“‘ å·²åŠ è½½ {len(df_boxes)} æ¡ YOLO æ£€æµ‹è®°å½•")

# è®¡ç®—æ¯ä¸ªæ£€æµ‹æ¡†å±äºå“ªä¸ª clip
df_boxes["clip_index"] = (df_boxes["frame_idx"] // clip_length_frames).astype(int)
df_boxes["clip_local_frame"] = (df_boxes["frame_idx"] % clip_length_frames).astype(int)

# åˆ†ç»„æ„å»º prompt å­—å…¸
instrument_boxes_by_clip = {}
for _, row in df_boxes.iterrows():
    clip_idx = int(row["clip_index"])
    frame_idx = int(row["clip_local_frame"])
    box = np.array([row["x1"], row["y1"], row["x2"], row["y2"]], dtype=np.float32)
    if clip_idx not in instrument_boxes_by_clip:
        instrument_boxes_by_clip[clip_idx] = []
    instrument_boxes_by_clip[clip_idx].append({
        "frame_idx": frame_idx,
        "box": box,
        "caption": row.get("caption", "")
    })

print(f"âœ… å·²ç”Ÿæˆ {len(instrument_boxes_by_clip)} ä¸ª clip çš„æç¤ºæ¡†ã€‚")

# ===================== 3. è®¾å¤‡é…ç½® =====================
os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"âœ… ä½¿ç”¨è®¾å¤‡: {device}")
if device.type == "cuda":
    torch.autocast("cuda", dtype=torch.bfloat16).__enter__()
    if torch.cuda.get_device_properties(0).major >= 8:
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True

# ===================== 4. å·¥å…·å‡½æ•° =====================
def adjust_box(box, w, h):
    """è°ƒæ•´æ¡†åæ ‡è‡³è§†é¢‘åˆ†è¾¨ç‡èŒƒå›´å†…"""
    adjusted = box.copy()
    adjusted[0] = max(0, min(adjusted[0], w - 1))
    adjusted[1] = max(0, min(adjusted[1], h - 1))
    adjusted[2] = max(adjusted[0] + 1, min(adjusted[2], w))
    adjusted[3] = max(adjusted[1] + 1, min(adjusted[3], h))
    return adjusted

def adjust_points(points, w, h):
    """è°ƒæ•´ç‚¹åæ ‡è‡³è§†é¢‘åˆ†è¾¨ç‡èŒƒå›´å†…"""
    adjusted = points.copy()
    adjusted[:, 0] = np.clip(adjusted[:, 0], 0, w - 1)  # xåæ ‡é™åˆ¶
    adjusted[:, 1] = np.clip(adjusted[:, 1], 0, h - 1)  # yåæ ‡é™åˆ¶
    return adjusted

# ===================== 5. ä¸»å¤„ç†å‡½æ•° =====================
def process_single_clip(clip_path, clip_index, predictor):
    clip_name = os.path.splitext(os.path.basename(clip_path))[0]
    output_dir = os.path.join(output_root, clip_name)
    os.makedirs(output_dir, exist_ok=True)
    print(f"\n===== å¼€å§‹å¤„ç†ç‰‡æ®µ {clip_index}: {clip_name} =====")

    cap = cv2.VideoCapture(clip_path)
    if not cap.isOpened():
        print(f"âŒ æ— æ³•æ‰“å¼€è§†é¢‘: {clip_path}")
        return
    fps = cap.get(cv2.CAP_PROP_FPS)
    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    print(f"ğŸ“¹ åˆ†è¾¨ç‡ {w}x{h}, å¸§æ•° {frame_count}, FPS={fps:.1f}")
    cap.release()

    # 1ï¸âƒ£ æå–è§†é¢‘å¸§ï¼ˆä¿ç•™BGRæ ¼å¼ï¼‰
    temp_dir = os.path.join(output_dir, "frames")
    os.makedirs(temp_dir, exist_ok=True)
    cap = cv2.VideoCapture(clip_path)
    for i in range(frame_count):
        ret, frame = cap.read()
        if not ret:
            continue
        cv2.imwrite(os.path.join(temp_dir, f"{i}.jpg"), frame)  # ä¸è½¬æ¢RGB
    cap.release()

    # 2ï¸âƒ£ åˆå§‹åŒ– SAM2 çŠ¶æ€
    inference_state = predictor.init_state(video_path=temp_dir)
    predictor.reset_state(inference_state)

    # 3ï¸âƒ£ æ·»åŠ çœ¼éƒ¨æç¤ºç‚¹ï¼ˆæ›¿æ¢åŸæ¥çš„æç¤ºæ¡†ï¼‰
    eye_center_adj = adjust_points(eye_center, w, h)  # è°ƒæ•´ç‚¹åæ ‡è‡³å›¾åƒèŒƒå›´å†…
    _, eye_obj_ids, _ = predictor.add_new_points_or_box(
        inference_state=inference_state,
        frame_idx=min(5, frame_count-1),  # åœ¨å‰5å¸§ä¸­æ·»åŠ æç¤º
        obj_id=1,
        points=eye_center_adj,  # ä½¿ç”¨ä¸­å¿ƒç‚¹ä½œä¸ºæç¤º
        labels=eye_label  # æ ‡è®°ä¸ºå‰æ™¯
    )
    print(f"âœ… æ·»åŠ çœ¼éƒ¨æç¤ºç‚¹ (Obj 1)")

    # 4ï¸âƒ£ æ·»åŠ  YOLO æ£€æµ‹æ¡†ä½œä¸ºå™¨æ¢°æç¤º
    instrument_prompts = []
    if clip_index in instrument_boxes_by_clip:
        for prompt in instrument_boxes_by_clip[clip_index]:
            box_adj = adjust_box(prompt["box"], w, h)
            _, obj_ids, _ = predictor.add_new_points_or_box(
                inference_state=inference_state,
                frame_idx=prompt["frame_idx"],
                obj_id=2,
                box=box_adj
            )
            instrument_prompts.append(prompt)
            print(f"âœ… æ·»åŠ å™¨æ¢°æç¤ºæ¡† (Obj 2) å¸§ {prompt['frame_idx']}")
    else:
        print(f"âš ï¸ å½“å‰ç‰‡æ®µ {clip_index} æ— æ£€æµ‹æ¡†æç¤º")

    # 5ï¸âƒ£ å¯è§†åŒ–æç¤ºæ¡†ä¸æç¤ºç‚¹
    print("ğŸ¨ å¯è§†åŒ–æç¤ºæ¡†ä¸­...")
    vis_frame_indices = {min(5, frame_count-1)} | {p["frame_idx"] for p in instrument_prompts}
    cap = cv2.VideoCapture(clip_path)
    for frame_idx in vis_frame_indices:
        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
        ret, frame = cap.read()
        if not ret:
            print(f"âš ï¸ å¯è§†åŒ–å¸§ {frame_idx} è¯»å–å¤±è´¥ï¼Œè·³è¿‡")
            continue
        
        # ç»˜åˆ¶çœ¼éƒ¨ä¸­å¿ƒç‚¹ï¼ˆç»¿è‰²ï¼‰
        center_x, center_y = int(eye_center_adj[0][0]), int(eye_center_adj[0][1])
        cv2.circle(frame, (center_x, center_y), 10, (0, 255, 0), -1)  # å®å¿ƒåœ†
        cv2.putText(frame, "Eye (Obj1)",
                    (center_x + 15, center_y),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
        
        # ç»˜åˆ¶å™¨æ¢°æ¡†ï¼ˆçº¢è‰²ï¼‰
        for prompt in instrument_prompts:
            if prompt["frame_idx"] == frame_idx:
                box = adjust_box(prompt["box"], w, h)
                cv2.rectangle(frame,
                              (int(box[0]), int(box[1])),
                              (int(box[2]), int(box[3])),
                              (0, 0, 255), 2)
                cv2.putText(frame, "Instrument (Obj2)",
                            (int(box[0]), int(box[1])-8),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)
        
        vis_save_path = os.path.join(output_dir, f"prompt_vis_frame_{frame_idx}.jpg")
        cv2.imwrite(vis_save_path, frame)
    cap.release()
    print(f"âœ… æç¤ºç‚¹å¯è§†åŒ–å®Œæˆï¼Œä¿å­˜è‡³ {output_dir}")

    # 6ï¸âƒ£ æ©ç ä¼ æ’­
    print("ğŸ”„ å¼€å§‹æ©ç ä¼ æ’­...")
    video_segments = {}
    for frame_idx, obj_ids, mask_logits in predictor.propagate_in_video(inference_state):
        masks = {}
        for i, obj_id in enumerate(obj_ids):
            mask = (mask_logits[i] > 0.5).squeeze(0).cpu().numpy()
            if mask.any():
                masks[obj_id] = mask
        if masks:
            video_segments[frame_idx] = masks

    # 7ï¸âƒ£ è¾“å‡ºåˆ†å‰²è§†é¢‘
    out_path = os.path.join(output_dir, f"{clip_name}_segmented.mp4")
    fourcc = cv2.VideoWriter_fourcc(*"mp4v")
    writer = cv2.VideoWriter(out_path, fourcc, fps, (w, h))
    colors = {1: (0, 255, 0), 2: (0, 0, 255)}
    for i in range(frame_count):
        frame = cv2.imread(os.path.join(temp_dir, f"{i}.jpg"))
        if i in video_segments:
            mask_layer = np.zeros_like(frame, dtype=np.uint8)
            for obj_id, mask in video_segments[i].items():
                mask = cv2.resize(mask.astype(np.float32), (w, h)) > 0.5
                mask_layer[mask] = colors.get(obj_id, (255, 255, 255))
            frame = cv2.addWeighted(frame, 0.6, mask_layer, 0.4, 0)
        writer.write(frame)
    writer.release()
    print(f"ğŸ¥ å·²ä¿å­˜åˆ†å‰²è§†é¢‘: {out_path}")

    shutil.rmtree(temp_dir)
    print(f"ğŸ—‘ï¸ æ¸…ç†ä¸´æ—¶å¸§ç›®å½• {temp_dir}")

# ===================== 6. æ‰¹é‡å¤„ç† =====================
if __name__ == "__main__":
    print("ğŸ”§ åŠ è½½SAM2æ¨¡å‹ä¸­...")
    predictor = build_sam2_video_predictor(model_cfg, checkpoint, device=device)
    print("âœ… SAM2åŠ è½½å®Œæˆ")

    clip_paths = sorted(glob.glob(os.path.join(clips_dir, "video4_clip_*.mp4")),
                        key=lambda x: int(os.path.splitext(os.path.basename(x))[0].split("_")[-1]))
    if not clip_paths:
        print(f"âŒ æœªæ‰¾åˆ°ä»»ä½•è§†é¢‘ç‰‡æ®µ: {clips_dir}")
        exit()

    for idx, clip_path in enumerate(clip_paths):
        process_single_clip(clip_path, idx, predictor)

    print(f"\nğŸ‰ å…¨éƒ¨ {len(clip_paths)} ä¸ªç‰‡æ®µå¤„ç†å®Œæˆï¼è¾“å‡ºç›®å½•: {output_root}")